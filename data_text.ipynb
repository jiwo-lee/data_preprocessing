{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53a5da72",
   "metadata": {},
   "source": [
    "## 설치파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7971555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.10.9\n",
    "#pip install nltk\n",
    "#pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d520d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jiwo2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b24b99",
   "metadata": {},
   "source": [
    "## Konlpy를 이용한 형태소 나누기  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc8f1f96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리된 텍스트: 나는 자연어 처리를 배우고 있습니다 이제 한글 텍스트 마이닝을 시작해보겠습니다\n",
      "단어 토큰화 결과: ['나', '는', '자연어', '처리', '를', '배우고', '있습니다', '이제', '한글', '텍스트', '마', '이닝', '을', '시작', '해보겠습니다']\n",
      "형태소 분석 결과: [('나', 'Noun'), ('는', 'Josa'), ('자연어', 'Noun'), ('처리', 'Noun'), ('를', 'Josa'), ('배우고', 'Verb'), ('있습니다', 'Adjective'), ('이제', 'Noun'), ('한글', 'Noun'), ('텍스트', 'Noun'), ('마', 'Noun'), ('이닝', 'Noun'), ('을', 'Josa'), ('시작', 'Noun'), ('해보겠습니다', 'Verb')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "import re\n",
    "\n",
    "# Okt 형태소 분석기 객체 생성\n",
    "okt = Okt()\n",
    "\n",
    "# 문장 텍스트\n",
    "text = \"나는 자연어 처리를 배우고 있습니다. 이제 한글 텍스트 마이닝을 시작해보겠습니다!\"\n",
    "\n",
    "# 1. 텍스트 전처리\n",
    "clean_text = re.sub(r'[^가-힣\\s]', '', text)  # 한글과 공백을 제외한 모든 문자 제거, \\s: 공백 문자 ^ :부정, 한글의 모든 음절:가-힣\n",
    "\n",
    "# 2. 단어 토큰화\n",
    "words = okt.morphs(clean_text)  # 문장을 형태소로 분해하여 단어 토큰화\n",
    "\n",
    "# 3. 형태소 분석\n",
    "pos_tags = okt.pos(clean_text)   # 형태소에 품사 태깅\n",
    "\n",
    "print(\"전처리된 텍스트:\", clean_text)\n",
    "print(\"단어 토큰화 결과:\", words)\n",
    "print(\"형태소 분석 결과:\", pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788df385",
   "metadata": {},
   "source": [
    "## 간단한 텍스트 전처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a72837e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요\n"
     ]
    }
   ],
   "source": [
    "# 빈칸 지우기\n",
    "text = \"  안녕하세요   \"\n",
    "clean_text = text.strip()\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a15e04e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'two', 'three']\n",
      "['one,two,three']\n",
      "hello world\n",
      "['안녕  ', ' 하잇']\n",
      "안녕  \n",
      " 하잇\n"
     ]
    }
   ],
   "source": [
    "sentence = \"one,two,three\"\n",
    "words = sentence.split(',')\n",
    "words2 = sentence.splitlines()\n",
    "print(words)\n",
    "print(words2)\n",
    "\n",
    "text = \"  hello world  \"\n",
    "clean_text = text.strip()\n",
    "print(clean_text)\n",
    "\n",
    "sentence = \"안녕  : 하잇\"\n",
    "words = sentence.strip().split(':')\n",
    "print(words)\n",
    "a, b =words\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435abac2",
   "metadata": {},
   "source": [
    "## Stopwords 사전구축 및 제거 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b39207d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 문장에는 단어1과 단어2가 존재합니다. 하지만 단어2는 매우 불필요한 단어 입니다.\n",
      "이 문장 과 존재\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "import re\n",
    "\n",
    "def remove_stopwords_nouns_only(sentence, stopwords_file):\n",
    "    #Okt 형태소 분석기 객체 생성\n",
    "    okt = Okt()\n",
    "\n",
    "    # 불용어 파일 읽기\n",
    "    with open(stopwords_file, 'r', encoding='utf-8') as f:\n",
    "        stopwords = f.read().splitlines()\n",
    "\n",
    "    #문장에서 명사만 추출\n",
    "    nouns = okt.nouns(sentence)\n",
    "\n",
    "    # 불용어가 아닌 명사만 필터링\n",
    "    filtered_nouns = []\n",
    "    for noun in nouns:\n",
    "        if noun not in stopwords:\n",
    "            filtered_nouns.append(noun)\n",
    "\n",
    "    #필터링된 명사들을 공백으로 구분하여 다시 문장 형태로 복원\n",
    "    cleaned_sentence = ' '.join(filtered_nouns)\n",
    "\n",
    "    return cleaned_sentence\n",
    "\n",
    "# 새로운 문장\n",
    "new_sentence = \"이 문장에는 단어1과 단어2가 존재합니다. 하지만 단어2는 매우 불필요한 단어 입니다.\"\n",
    "# 불용어 리스트가 저장된 텍스트 파일 경로\n",
    "stopwords_file = 'stopwords.txt'  # 예시를 위해 실제 경로로 수정 필요\n",
    "\n",
    "# 불용어를 제거한 결과 출력\n",
    "cleaned_sentence = remove_stopwords_nouns_only(new_sentence, stopwords_file)\n",
    "print(new_sentence)\n",
    "print(cleaned_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d444d35",
   "metadata": {},
   "source": [
    "## Stopwords를 활용한 텍스트 데이터 명사추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49847b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# 불용어 파일 읽기 함수\n",
    "def read_stopwords(stopwords_file):\n",
    "    with open(stopwords_file, 'r', encoding='utf-8') as f:\n",
    "        stopwords = f.read().splitlines()\n",
    "    return stopwords\n",
    "\n",
    "# 명사 추출 및 불용어 제거 함수\n",
    "def remove_stopwords_nouns(sentence, stopwords):\n",
    "    okt = Okt()\n",
    "    nouns = okt.nouns(sentence)\n",
    "\n",
    "    # 리스트 컴프리헨션 대신 for문 사용\n",
    "    filtered_nouns = []\n",
    "    for noun in nouns:\n",
    "        if noun not in stopwords:\n",
    "            filtered_nouns.append(noun)\n",
    "\n",
    "    return ' '.join(filtered_nouns)\n",
    "\n",
    "# CSV 파일 처리 함수\n",
    "def process_csv(input_csv, stopwords_file, output_csv):\n",
    "    # 불용어 목록 읽기\n",
    "    stopwords = read_stopwords(stopwords_file)\n",
    "\n",
    "    # 입력 CSV 파일 읽기\n",
    "    df = pd.read_csv(input_csv, encoding='utf-8')\n",
    "    df['text'] = df['text'].fillna('').astype(str)\n",
    "    # df['text']의 각 문장에 대해 명사 추출 및 불용어 제거 처리\n",
    "    # apply 대신 for문 사용\n",
    "    processed_texts = []\n",
    "    for sentence in df['text']:\n",
    "        processed_text = remove_stopwords_nouns(sentence, stopwords)\n",
    "        processed_texts.append(processed_text)\n",
    "\n",
    "    df['processed_text'] = processed_texts\n",
    "\n",
    "    # 처리된 데이터를 새로운 CSV 파일로 저장\n",
    "    df.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "\n",
    "# 사용 예시\n",
    "input_csv = 'input_sample.csv'  # 원본 CSV 파일 경로\n",
    "stopwords_file = 'stopwords.txt'  # 불용어 리스트 파일 경로\n",
    "output_csv = 'output_sample.csv'  # 결과를 저장할 새 CSV 파일 경로\n",
    "\n",
    "# CSV 파일 처리 실행\n",
    "process_csv(input_csv, stopwords_file, output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e77485",
   "metadata": {},
   "source": [
    "## 추출명사를 활용한 감성분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d318ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어와 점수 매핑을 생성하는 함수\n",
    "def create_word_score_mapping(filepath):\n",
    "    word_score_mapping = {}\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            word, score = line.strip().split('+')\n",
    "            word_score_mapping[word] = int(score)\n",
    "    return word_score_mapping\n",
    "\n",
    "# 긍정과 부정 단어 점수 파일\n",
    "positive_filepath = 'positive_words.txt'\n",
    "negative_filepath = 'negative_words.txt'\n",
    "\n",
    "# 매핑 생성\n",
    "positive_scores = create_word_score_mapping(positive_filepath)\n",
    "negative_scores = create_word_score_mapping(negative_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711f54f4",
   "metadata": {},
   "source": [
    "### 1. 텍스트 입력기반 감성분석 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93ce8056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장의 감정 분석 결과: 부정, 점수: -2\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "def analyze_sentiment(sentence, positive_scores, negative_scores):\n",
    "    okt = Okt()\n",
    "    score = 0\n",
    "\n",
    "    # 문장에서 명사 추출\n",
    "    words = okt.morphs(sentence)\n",
    "\n",
    "    # 각 단어의 점수 계산\n",
    "    for word in words:\n",
    "        if word in positive_scores:\n",
    "            score += positive_scores[word]\n",
    "        elif word in negative_scores:\n",
    "            score -= negative_scores[word]\n",
    "\n",
    "    # 최종 점수를 기반으로 감정 결정\n",
    "    if score > 0:\n",
    "        return \"긍정\", score\n",
    "    elif score < 0:\n",
    "        return \"부정\", score\n",
    "    else:\n",
    "        return \"중립\", score\n",
    "\n",
    "# 사용 예시\n",
    "sentence = \"이 영화는 정말 재미와 흥미를 나에게 주었고, 스케일이 엄청나다.\"\n",
    "sentence2 = \"이 영화는 주인공의 연기가 노잼이여서 싫다. 하지만 흥미는 있었다.\"\n",
    "sentiment, total_score = analyze_sentiment(sentence2, positive_scores, negative_scores)\n",
    "print((\"문장의 감정 분석 결과: {}, 점수: {}\").format(sentiment,total_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5201678e",
   "metadata": {},
   "source": [
    "### 2.  csv파일을 활용한 감성분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65771c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# 단어와 점수 매핑을 생성하는 함수\n",
    "def create_word_score_mapping(filepath):\n",
    "    word_score_mapping = {}\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            word, score = line.strip().split('+')\n",
    "            word_score_mapping[word] = int(score)\n",
    "    return word_score_mapping\n",
    "\n",
    "# 감정 점수 계산 함수\n",
    "def calculate_sentiment_score(sentence, positive_scores, negative_scores):\n",
    "    okt = Okt()\n",
    "    score = 0\n",
    "    words = okt.morphs(sentence)\n",
    "    for word in words:\n",
    "        if word in positive_scores:\n",
    "            score += positive_scores[word]\n",
    "        elif word in negative_scores:\n",
    "            score -= negative_scores[word]\n",
    "    return score\n",
    "\n",
    "# CSV 파일 처리 및 감정 점수 계산\n",
    "def process_csv_with_sentiment(input_csv, positive_filepath, negative_filepath, output_csv):\n",
    "    positive_scores = create_word_score_mapping(positive_filepath)\n",
    "    negative_scores = create_word_score_mapping(negative_filepath)\n",
    "\n",
    "    df = pd.read_csv(input_csv, encoding=\"utf-8\")\n",
    "    df['processed_text'] = df['processed_text'].fillna('').astype(str)\n",
    "    # 감정 점수를 저장할 빈 리스트 초기화\n",
    "    scores = []\n",
    "\n",
    "    # for문을 사용하여 각 텍스트의 감정 점수 계산\n",
    "    for text in df['text']:\n",
    "        score = calculate_sentiment_score(text, positive_scores, negative_scores)\n",
    "        scores.append(score)\n",
    "\n",
    "    # 계산된 감정 점수를 DataFrame의 새로운 열로 추가\n",
    "    df['score'] = scores\n",
    "\n",
    "    # 결과를 새로운 CSV 파일로 저장\n",
    "    df.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "\n",
    "# 파일 경로 설정\n",
    "input_csv = 'output_sample.csv'  # 원본 CSV 파일 경로\n",
    "positive_filepath = 'positive_words.txt'  # 긍정 단어 점수 파일 경로\n",
    "negative_filepath = 'negative_words.txt'  # 부정 단어 점수 파일 경로\n",
    "output_csv = 'output_scores.csv'  # 결과 CSV 파일 경로\n",
    "\n",
    "# CSV 파일 처리 실행\n",
    "process_csv_with_sentiment(input_csv, positive_filepath, negative_filepath, output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fe1def",
   "metadata": {},
   "source": [
    "## 워드클라우드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d099817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.9.3-cp311-cp311-win_amd64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\jiwo2\\appdata\\local\\anaconda3\\envs\\data2\\lib\\site-packages (from wordcloud) (1.26.4)\n",
      "Requirement already satisfied: pillow in c:\\users\\jiwo2\\appdata\\local\\anaconda3\\envs\\data2\\lib\\site-packages (from wordcloud) (10.3.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\jiwo2\\appdata\\local\\anaconda3\\envs\\data2\\lib\\site-packages (from wordcloud) (3.8.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jiwo2\\appdata\\local\\anaconda3\\envs\\data2\\lib\\site-packages (from matplotlib->wordcloud) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jiwo2\\appdata\\local\\anaconda3\\envs\\data2\\lib\\site-packages (from matplotlib->wordcloud) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jiwo2\\appdata\\local\\anaconda3\\envs\\data2\\lib\\site-packages (from matplotlib->wordcloud) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\jiwo2\\appdata\\local\\anaconda3\\envs\\data2\\lib\\site-packages (from matplotlib->wordcloud) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jiwo2\\appdata\\local\\anaconda3\\envs\\data2\\lib\\site-packages (from matplotlib->wordcloud) (24.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\jiwo2\\appdata\\local\\anaconda3\\envs\\data2\\lib\\site-packages (from matplotlib->wordcloud) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\jiwo2\\appdata\\local\\anaconda3\\envs\\data2\\lib\\site-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jiwo2\\appdata\\local\\anaconda3\\envs\\data2\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Downloading wordcloud-1.9.3-cp311-cp311-win_amd64.whl (300 kB)\n",
      "   ---------------------------------------- 0.0/300.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/300.2 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/300.2 kB ? eta -:--:--\n",
      "   ----- --------------------------------- 41.0/300.2 kB 393.8 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 235.5/300.2 kB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 300.2/300.2 kB 1.9 MB/s eta 0:00:00\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.9.3\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55eb00cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\jiwo2\\\\Desktop\\\\data1\\\\데이터전처리_텍스트\\\\NanumFontSetup_TTF_SQUARE\\\\NanumSquare_acB'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\data3\\lib\\site-packages\\PIL\\ImageFont.py:237\u001b[0m, in \u001b[0;36mFreeTypeFont.__init__\u001b[1;34m(self, font, size, index, encoding, layout_engine)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 237\u001b[0m     \u001b[43mfont_bytes_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mascii\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# FreeType cannot load fonts with non-ASCII characters on Windows\u001b[39;00m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# So load it into memory first\u001b[39;00m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xeb in position 29: ordinal not in range(128)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m min_word_frequency \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# 워드 클라우드 생성 함수 호출\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m \u001b[43mcreate_wordcloud_from_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_word_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_word_frequency\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 25\u001b[0m, in \u001b[0;36mcreate_wordcloud_from_df\u001b[1;34m(df, min_word_length, min_word_frequency)\u001b[0m\n\u001b[0;32m     22\u001b[0m filtered_word_counts \u001b[38;5;241m=\u001b[39m {word: freq \u001b[38;5;28;01mfor\u001b[39;00m word, freq \u001b[38;5;129;01min\u001b[39;00m word_counts\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(word) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m min_word_length \u001b[38;5;129;01mand\u001b[39;00m freq \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m min_word_frequency}\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# 워드 클라우드 생성\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m wordcloud \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfont_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNanumFontSetup_TTF_SQUARE/NanumSquare_acB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwhite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_from_frequencies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_word_counts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# 워드 클라우드 시각화\u001b[39;00m\n\u001b[0;32m     28\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\data3\\lib\\site-packages\\wordcloud\\wordcloud.py:453\u001b[0m, in \u001b[0;36mWordCloud.generate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    451\u001b[0m     font_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 453\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_from_frequencies\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfrequencies\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mmax_font_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;66;03m# find font sizes\u001b[39;00m\n\u001b[0;32m    456\u001b[0m     sizes \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout_]\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\data3\\lib\\site-packages\\wordcloud\\wordcloud.py:506\u001b[0m, in \u001b[0;36mWordCloud.generate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;66;03m# try to find a position\u001b[39;00m\n\u001b[1;32m--> 506\u001b[0m font \u001b[38;5;241m=\u001b[39m \u001b[43mImageFont\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtruetype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfont_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfont_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;66;03m# transpose font optionally\u001b[39;00m\n\u001b[0;32m    508\u001b[0m transposed_font \u001b[38;5;241m=\u001b[39m ImageFont\u001b[38;5;241m.\u001b[39mTransposedFont(\n\u001b[0;32m    509\u001b[0m     font, orientation\u001b[38;5;241m=\u001b[39morientation)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\data3\\lib\\site-packages\\PIL\\ImageFont.py:807\u001b[0m, in \u001b[0;36mtruetype\u001b[1;34m(font, size, index, encoding, layout_engine)\u001b[0m\n\u001b[0;32m    804\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m FreeTypeFont(font, size, index, encoding, layout_engine)\n\u001b[0;32m    806\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 807\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfreetype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfont\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_path(font):\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\data3\\lib\\site-packages\\PIL\\ImageFont.py:804\u001b[0m, in \u001b[0;36mtruetype.<locals>.freetype\u001b[1;34m(font)\u001b[0m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfreetype\u001b[39m(font):\n\u001b[1;32m--> 804\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFreeTypeFont\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfont\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayout_engine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\data3\\lib\\site-packages\\PIL\\ImageFont.py:241\u001b[0m, in \u001b[0;36mFreeTypeFont.__init__\u001b[1;34m(self, font, size, index, encoding, layout_engine)\u001b[0m\n\u001b[0;32m    237\u001b[0m     font_bytes_path\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mascii\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# FreeType cannot load fonts with non-ASCII characters on Windows\u001b[39;00m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# So load it into memory first\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfont\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    242\u001b[0m         load_from_bytes(f)\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\jiwo2\\\\Desktop\\\\data1\\\\데이터전처리_텍스트\\\\NanumFontSetup_TTF_SQUARE\\\\NanumSquare_acB'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 데이터프레임의 'text' 열로부터 워드 클라우드 생성하는 함수\n",
    "def create_wordcloud_from_df(df, min_word_length=2, min_word_frequency=1):\n",
    "    # 'text' 열의 모든 텍스트를 하나의 문자열로 결합하기 위한 빈 문자열 초기화\n",
    "    text = \"\"\n",
    "\n",
    "    # for문을 사용하여 각 행의 'text' 열 값을 하나의 문자열로 결합\n",
    "    for review in df['processed_text']:\n",
    "        text += str(review) + \" \"  # 각 리뷰 사이에 공백 추가\n",
    "\n",
    "    # 마지막에 추가된 공백 제거\n",
    "    text = text.strip()\n",
    "    \n",
    "    # 단어 빈도수 계산\n",
    "    word_counts = Counter(text.split())\n",
    "    \n",
    "    # 최소 단어 길이와 최소 단어 빈도에 따라 필터링\n",
    "    filtered_word_counts = {word: freq for word, freq in word_counts.items() if len(word) >= min_word_length and freq >= min_word_frequency}\n",
    "    \n",
    "    # 워드 클라우드 생성\n",
    "    wordcloud = WordCloud(font_path='NanumFontSetup_TTF_SQUARE/NanumSquare_acB', background_color='white', width=800, height=400).generate_from_frequencies(filtered_word_counts)\n",
    "\n",
    "    # 워드 클라우드 시각화\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# CSV 파일 로드\n",
    "csv_file_path = 'output_sample.csv'  # CSV 파일 경로를 여기에 입력\n",
    "df = pd.read_csv(csv_file_path, encoding=\"utf-8\")\n",
    "df['processed_text'] = df['processed_text'].astype(str)\n",
    "\n",
    "# 최소 단어 길이 및 빈도 설정\n",
    "min_word_length = 2\n",
    "min_word_frequency = 2\n",
    "\n",
    "# 워드 클라우드 생성 함수 호출\n",
    "create_wordcloud_from_df(df, min_word_length, min_word_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bfb11f",
   "metadata": {},
   "source": [
    "## 동의어 사전을 활용한 동의어 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b92b430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    가라고 해서 가려고 하니 가지말라 해서 안가려고 하니 계속가라 하더라\n",
      "Name: text_final, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 동의어 바꾸기 규칙 파일을 로드하는 함수\n",
    "def load_replacement_rules(rules_filepath):\n",
    "    replacement_rules = {}\n",
    "    with open(rules_filepath, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            target, synonyms = line.strip().split(':')\n",
    "            target = target.strip()\n",
    "            synonyms = [synonym.strip() for synonym in synonyms.split(',')]\n",
    "            for synonym in synonyms:\n",
    "                replacement_rules[synonym] = target\n",
    "    return replacement_rules\n",
    "\n",
    "# 텍스트 내 단어를 규칙에 따라 바꾸는 함수\n",
    "def replace_words(text, replacement_rules):\n",
    "    for synonym, target in replacement_rules.items():\n",
    "        text = text.replace(synonym, target)\n",
    "    return text\n",
    "\n",
    "# DataFrame 로드 (예시)\n",
    "df = pd.read_csv('text_dict.csv', encoding=\"utf-8-sig\")\n",
    "df['text'] = df['text'].astype(str)\n",
    "\n",
    "# 단어 변환 규칙 파일 로드\n",
    "replacement_rules = load_replacement_rules('word_replacement_rules.txt')\n",
    "\n",
    "# 변환된 텍스트를 저장할 리스트\n",
    "replaced_texts = []\n",
    "\n",
    "# 각 텍스트에 대해 변환 수행\n",
    "for text in df['text']:\n",
    "    replaced_text = replace_words(text, replacement_rules)\n",
    "    replaced_texts.append(replaced_text)\n",
    "\n",
    "# 변환된 텍스트를 DataFrame의 새로운 열로 추가\n",
    "df['text_final'] = replaced_texts\n",
    "\n",
    "# 변환된 텍스트 출력\n",
    "print(df['text_final'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54979cfe",
   "metadata": {},
   "source": [
    "### 동의어 바꾸기 csv파일에 저장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9a5f2f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'word_replacement_rules2.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# 단어 변환 규칙 파일 로드\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m replacement_rules \u001b[38;5;241m=\u001b[39m \u001b[43mload_replacement_rules\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mword_replacement_rules2.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# 변환된 텍스트를 저장할 리스트\u001b[39;00m\n\u001b[0;32m     29\u001b[0m replaced_texts \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[16], line 6\u001b[0m, in \u001b[0;36mload_replacement_rules\u001b[1;34m(rules_filepath)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_replacement_rules\u001b[39m(rules_filepath):\n\u001b[0;32m      5\u001b[0m     replacement_rules \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrules_filepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[0;32m      8\u001b[0m             target, synonyms \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\data3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'word_replacement_rules2.txt'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 동의어 바꾸기 규칙 파일을 로드하는 함수\n",
    "def load_replacement_rules(rules_filepath):\n",
    "    replacement_rules = {}\n",
    "    with open(rules_filepath, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            target, synonyms = line.strip().split(':')\n",
    "            target = target.strip()\n",
    "            synonyms = [synonym.strip() for synonym in synonyms.split(',')]\n",
    "            for synonym in synonyms:\n",
    "                replacement_rules[synonym] = target\n",
    "    return replacement_rules\n",
    "\n",
    "# 텍스트 내 단어를 규칙에 따라 바꾸는 함수\n",
    "def replace_words(text, replacement_rules):\n",
    "    for synonym, target in replacement_rules.items():\n",
    "        text = text.replace(synonym, target)\n",
    "    return text\n",
    "\n",
    "# DataFrame 로드 (예시)\n",
    "df = pd.read_csv('input_sample.csv', encoding=\"utf-8-sig\")\n",
    "df['text'] = df['text'].astype(str)\n",
    "\n",
    "# 단어 변환 규칙 파일 로드\n",
    "replacement_rules = load_replacement_rules('word_replacement_rules2.txt')\n",
    "\n",
    "# 변환된 텍스트를 저장할 리스트\n",
    "replaced_texts = []\n",
    "\n",
    "# 각 텍스트에 대해 변환 수행\n",
    "for text in df['text']:\n",
    "    replaced_text = replace_words(text, replacement_rules)\n",
    "    replaced_texts.append(replaced_text)\n",
    "\n",
    "# 변환된 텍스트를 DataFrame의 새로운 열로 추가\n",
    "df['text_final'] = replaced_texts\n",
    "\n",
    "# 변환된 결과를 새로운 CSV 파일로 저장\n",
    "output_replacements = 'output_replacements.csv'\n",
    "df.to_csv(output_replacements, index=False, encoding='utf-8-sig')\n",
    "\n",
    "# 변환된 텍스트 출력\n",
    "print(df['text_final'])\n",
    "\n",
    "# 최종 변환된 텍스트를 별도의 CSV 파일로 저장\n",
    "final_text_df = pd.DataFrame(df)\n",
    "final_text_csv = 'final_text.csv'\n",
    "final_text_df.to_csv(final_text_csv, index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be170b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    아니 싫다 할머니 이 옷 노란게 밖에 나가면 많다\n",
      "Name: text_final, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 동의어 바꾸기 규칙 파일을 로드하는 함수\n",
    "def load_replacement_rules(rules_filepath):\n",
    "    replacement_rules = {}\n",
    "    with open(rules_filepath, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            target, synonyms = line.strip().split(':')\n",
    "            target = target.strip()\n",
    "            synonyms = [synonym.strip() for synonym in synonyms.split(',')]\n",
    "            for synonym in synonyms:\n",
    "                replacement_rules[synonym] = target\n",
    "    return replacement_rules\n",
    "\n",
    "# 텍스트 내 단어를 규칙에 따라 바꾸는 함수\n",
    "def replace_words(text, replacement_rules):\n",
    "    for synonym, target in replacement_rules.items():\n",
    "        text = text.replace(synonym, target)\n",
    "    return text\n",
    "\n",
    "# DataFrame 로드 (예시)\n",
    "df = pd.read_csv('text_dict1.csv', encoding=\"utf-8-sig\")\n",
    "df['text'] = df['text'].astype(str)\n",
    "\n",
    "# 단어 변환 규칙 파일 로드\n",
    "replacement_rules = load_replacement_rules('word_replacement_rules.txt')\n",
    "\n",
    "# 변환된 텍스트를 저장할 리스트\n",
    "replaced_texts = []\n",
    "\n",
    "# 각 텍스트에 대해 변환 수행\n",
    "for text in df['text']:\n",
    "    replaced_text = replace_words(text, replacement_rules)\n",
    "    replaced_texts.append(replaced_text)\n",
    "\n",
    "# 변환된 텍스트를 DataFrame의 새로운 열로 추가\n",
    "df['text_final'] = replaced_texts\n",
    "\n",
    "# 변환된 텍스트 출력\n",
    "print(df['text_final'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d8d98c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data3",
   "language": "python",
   "name": "data3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
